{"cells":[{"cell_type":"markdown","metadata":{"id":"KDVDq4R4cQqN"},"source":["Import and setup some auxiliary functions"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4902,"status":"ok","timestamp":1665621313006,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"qHPwL1QYcQqU"},"outputs":[],"source":["# Don't edit this cell\n","import torch\n","from torchvision import transforms, datasets\n","import numpy as np\n","import timeit\n","from collections import OrderedDict\n","from pprint import pformat\n","from torch.utils.data.sampler import *\n","from tqdm import tqdm\n","import time\n","from prettytable import PrettyTable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","def compute_score(acc, min_thres, max_thres):\n","    if acc <= min_thres:\n","        base_score = 0.0\n","    elif acc >= max_thres:\n","        base_score = 100.0\n","    else:\n","        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n","                     * 100\n","    return base_score\n","\n","\n","def run(algorithm, dataset_name, filename):\n","    predicted_test_labels, gt_labels, run_time, parameters_count = algorithm(dataset_name)\n","    if predicted_test_labels is None or gt_labels is None:\n","      return (0, 0, 0, 0)\n","\n","    correct = 0\n","    total = 0\n","    for label, prediction in zip(gt_labels, predicted_test_labels):\n","      total += label.size(0)\n","      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n","      \n","    accuracy = float(correct) / total\n","    \n","    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n","    return (correct, accuracy, run_time, parameters_count)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665621313007,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"17Mjmw05cQq0"},"outputs":[],"source":["def load_data(dataset_name, device, config):\n","    \"\"\"\n","    loads cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data\n","    \"\"\"\n","\n","    training = datasets.CIFAR10(root='./data', train=True, download=True, transform=config['transforms'])\n","    test = datasets.CIFAR10(root='./data', train=False, download=True, transform=config['transforms'])\n","    \n","    trainingLoader = torch.utils.data.DataLoader(training,\n","                                                 batch_size=config['batch_size'],\n","                                                 sampler=SubsetRandomSampler(list(range(len(training) - 5000))),\n","                                                 num_workers=2)\n","    validationLoader = torch.utils.data.DataLoader(training,\n","                                                   batch_size=config['batch_size'],\n","                                                   sampler=SubsetRandomSampler(list(range(len(training) - 5000, len(training)))),\n","                                                   num_workers=2)\n","    testLoader = torch.utils.data.DataLoader(test, num_workers=2)\n","\n","    return trainingLoader, validationLoader, testLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665621313007,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"f2PKEVDSeeJ9"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.seq = nn.Sequential(\n","            nn.Conv2d(3,32,3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Conv2d(32,32,3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Conv2d(32,32,3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.ConvTranspose2d(32,64,3),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(1024,10)\n","        )\n","\n","        # He Initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m,nn.Linear):\n","                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","                m.bias.detach().zero_()\n","                \n","    def forward(self, x):\n","        return self.seq(x)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665621313008,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"Q3GcXjw5emv0"},"outputs":[],"source":["def train(train_dataloader, valid_dataloader, device, config):\n","  net = Net().to(device)\n","  opt = torch.optim.Adam(net.parameters(), config['lr'], weight_decay=config['regular_constant'])\n","\n","  # print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n","  def validate():\n","    net.eval()\n","    loss = correct = 0\n","    \n","    with torch.no_grad():\n","      for data, target in valid_dataloader:\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        output = net(data)\n","        pred = output.data.max(1, keepdim=True)[1]\n","\n","        loss += F.cross_entropy(output, target, reduction='sum').item()\n","        correct += pred.eq(target.data.view_as(pred)).sum()\n","\n","    loss /= len(valid_dataloader.dataset)\n","\n","    print(f'Validation Set: AVG LOSS: {loss:.4f} ACC: {correct}/{len(valid_dataloader.dataset)} {(100. * correct / len(valid_dataloader.dataset)):.2f}%')\n","\n","  def _train():\n","    net.train()\n","\n","    bar = tqdm(train_dataloader, ncols=100, position=0, leave=True)\n","    avgLoss = 0\n","\n","    for i, (data, target) in enumerate(bar):\n","      data = data.to(device)\n","      target = target.to(device)\n","      opt.zero_grad()\n","\n","      output = net(data)\n","      \n","      loss = F.cross_entropy(output, target)\n","      avgLoss += (loss.item() - avgLoss) / (i+1)\n","      loss.backward()\n","      opt.step()\n","\n","      bar.set_description(f'Train Loss: {loss.item():.6f} AVG Loss: {avgLoss:.6f}')\n","\n","  # validate()\n","  for _ in range(config['num_epochs']):\n","    _train()\n","\n","  return net"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1665621329214,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"U9dIUkH6e5T2"},"outputs":[],"source":["def test(model, test_dataloader, device):\n","  test_predictions = []\n","  true_labels = []\n","\n","  model.eval()\n","\n","  for data, target in test_dataloader:\n","    data = data.to(device)\n","    target = target.to(device)\n","\n","    output = model(data)\n","    pred = output.data.max(1, keepdim=True)[1]\n","\n","    test_predictions.append(pred)\n","    true_labels.append(target)\n","\n","\n","  return test_predictions, true_labels\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","        total_params+=params\n","    # uncomment below codes for your debugging\n","    # print(table)\n","    # print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","\n","def run_NN(dataset_name):\n","    # set parameters cifar10\n","  config = {\n","        'lr': 0.001,\n","        'num_epochs': 10,\n","        'batch_size': 128,\n","        'num_classes': 10,\n","        'regular_constant': 0.001,\n","        'transforms': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) }\n","    \n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  train_dataloader, valid_dataloader, test_dataloader = load_data(dataset_name, device, config)\n","  \n","  model = train(train_dataloader, valid_dataloader, device, config)\n","  parameters_count = count_parameters(model)\n","\n","  device = torch.device(\"cpu\")\n","  start_time = timeit.default_timer()\n","  assert test_dataloader.batch_size == 1, 'Error: You should use use batch size = 1 for the test loader.'\n","  preds, labels = test(model.to(device), test_dataloader, device)\n","  end_time = timeit.default_timer()\n","  \n","\n","  test_time = (end_time - start_time)\n","  print(\"Total run time of testing the model: \", test_time , \" seconds.\")\n","    \n","  return preds, labels, test_time, parameters_count"]},{"cell_type":"markdown","metadata":{"id":"GNNgL7C7cQq-"},"source":["Main loop. Run time and total score will be shown below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qf9iL8S_cQrB","outputId":"62ab3279-7b7a-4198-e40e-910fc103d997"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["Train Loss: 1.384118 AVG Loss: 1.595266: 100%|████████████████████| 352/352 [00:51<00:00,  6.82it/s]\n","Train Loss: 0.994363 AVG Loss: 1.270130: 100%|████████████████████| 352/352 [00:50<00:00,  6.92it/s]\n","Train Loss: 1.052449 AVG Loss: 1.143127: 100%|████████████████████| 352/352 [00:51<00:00,  6.81it/s]\n","Train Loss: 1.114309 AVG Loss: 1.053532: 100%|████████████████████| 352/352 [00:50<00:00,  7.02it/s]\n","Train Loss: 0.991149 AVG Loss: 0.992438: 100%|████████████████████| 352/352 [00:49<00:00,  7.11it/s]\n","Train Loss: 0.877121 AVG Loss: 0.937386: 100%|████████████████████| 352/352 [00:48<00:00,  7.26it/s]\n","Train Loss: 1.011319 AVG Loss: 0.892783: 100%|████████████████████| 352/352 [00:48<00:00,  7.21it/s]\n","Train Loss: 0.899972 AVG Loss: 0.854884: 100%|████████████████████| 352/352 [00:48<00:00,  7.20it/s]\n","Train Loss: 0.834714 AVG Loss: 0.814014: 100%|████████████████████| 352/352 [00:48<00:00,  7.33it/s]\n","Train Loss: 0.914007 AVG Loss: 0.783570: 100%|████████████████████| 352/352 [00:49<00:00,  7.09it/s]\n"]}],"source":["# Don't edit this cell\n","def run_on_dataset(dataset_name, filename):\n","    min_thres = 0.55\n","    max_thres = 0.65\n","\n","    correct_predict, accuracy, run_time, parameters_count = run(run_NN, dataset_name, filename)\n","\n","    score = compute_score(accuracy, min_thres, max_thres)\n","    if parameters_count > 50000:\n","      score = max(0, score - 25)\n","    result = OrderedDict(\n","                  score=score,\n","                  correct_predict=correct_predict,\n","                  accuracy=accuracy,\n","                  run_time=run_time,\n","                  parameters_count=parameters_count)\n","    return result, score\n","\n","\n","def main():\n","    filenames = { \"CIFAR10\": \"predictions_cifar10_YoussefIsmail_1616494.txt\"}\n","    result_all = OrderedDict()\n","    scores = []\n","    for dataset_name in [\"CIFAR10\"]:\n","        result_all, this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n","    with open('result.txt', 'w') as f:\n","        f.writelines(pformat(result_all, indent=4))\n","    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n","\n","\n","main()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
