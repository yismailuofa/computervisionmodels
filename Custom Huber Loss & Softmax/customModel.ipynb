{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3014,"status":"ok","timestamp":1663693505161,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"Zp3BetP-d6cB"},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","from torch.autograd import Function\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"MJpovSL8d_-l"},"source":["## Huber loss function\n","https://en.wikipedia.org/wiki/Huber_loss"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1663693505162,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"GTp4nNf9d-zg"},"outputs":[],"source":["# A loss function measures distance between a predicted and a target tensor\n","# An implementation of Huber loss function is given below\n","# We will make use of this loss function in gradient descent optimization\n","def Huber_Loss(input,delta):\n","  m = (torch.abs(input)<=delta).detach().float()\n","  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n","  return output"]},{"cell_type":"markdown","metadata":{"id":"zZoxXPadgk-O"},"source":["# Test Huber loss with a couple of different examples"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1663693505162,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"KYO_KmUQfmnm","outputId":"933a4672-8d3f-4e64-8f0d-531077250d9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.3  2.  -3.1]\n"," [ 0.5  9.2  0.1]]\n","12.975\n","[0.3 2. ]\n","1.545\n"]}],"source":["a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n","print(a.numpy())\n","ha = Huber_Loss(a,1.0)\n","print(ha.numpy())\n","\n","b = torch.tensor([0.3, 2.0])\n","print(b.numpy())\n","hb = Huber_Loss(b,1.0)\n","print(hb.numpy())"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663693505162,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"NLxQgQaD7Krq"},"outputs":[],"source":["def gradient_descent(var,optimizer,softmax,loss,target,nIter,nPrint):\n","  for i in range(nIter):\n","    z = softmax(var)\n","    f = loss(z-target,1.0)\n","    optimizer.zero_grad()\n","    f.backward()\n","    optimizer.step()\n","    if i%nPrint==0:\n","      with np.printoptions(precision=3, suppress=True):\n","        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454,"status":"ok","timestamp":1663693505609,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"AzRgWv_NiIeQ","outputId":"4e64f61a-4b65-4ccb-9cfa-dea1c75eb33c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Iteration: 0 Variable: [0.264 0.06  0.053 0.03  0.067 0.044 0.015 0.099 0.093 0.275] Loss: 0.536443\n","Iteration: 100 Variable: [0.008 0.006 0.945 0.004 0.007 0.005 0.002 0.008 0.008 0.008] Loss: 0.001678\n","Iteration: 200 Variable: [0.006 0.005 0.958 0.003 0.005 0.004 0.002 0.006 0.006 0.006] Loss: 0.000967\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"name":"stdout","output_type":"stream","text":["Iteration: 300 Variable: [0.005 0.004 0.965 0.002 0.004 0.003 0.001 0.005 0.005 0.005] Loss: 0.000683\n","Iteration: 400 Variable: [0.004 0.003 0.969 0.002 0.004 0.003 0.001 0.004 0.004 0.004] Loss: 0.000527\n","Iteration: 500 Variable: [0.004 0.003 0.972 0.002 0.003 0.003 0.001 0.004 0.004 0.004] Loss: 0.000430\n","Iteration: 600 Variable: [0.004 0.003 0.975 0.002 0.003 0.002 0.001 0.004 0.004 0.004] Loss: 0.000362\n","Iteration: 700 Variable: [0.003 0.003 0.976 0.002 0.003 0.002 0.001 0.003 0.003 0.003] Loss: 0.000313\n","Iteration: 800 Variable: [0.003 0.003 0.978 0.002 0.003 0.002 0.001 0.003 0.003 0.003] Loss: 0.000276\n","Iteration: 900 Variable: [0.003 0.002 0.979 0.001 0.003 0.002 0.001 0.003 0.003 0.003] Loss: 0.000246\n"]}],"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(\"Target 1-hot vector:\",y.numpy())\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,F.softmax,Huber_Loss,y,1000,100)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663693505609,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"7dANLmUENEaH","outputId":"b1276c8d-da23-48f7-f411-89a91a2345ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["When x is tensor([ 3., -1.,  0.,  1.]) function value is 11.0\n","Gradient of f at x: tensor([ 6., -2.,  0.,  2.])\n"]}],"source":["# Inherit from torch.autograd.Function\n","class My_f(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, x):\n","        f = torch.sum(x**2)\n","        ctx.save_for_backward(x,torch.tensor(2.0)) # note that the constant 2.0 is cast as a pytorch tensor before saving\n","        return f\n","\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        # retrieve saved tensors and use them in derivative calculation\n","        x,two = ctx.saved_tensors\n","        # Return Jacobian-vector product (chain rule)\n","        input_grad = two*x*output_grad\n","                \n","        return input_grad\n","\n","x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","my_fval = My_f.apply(x)\n","print(\"When x is\", x.data, \"function value is\",my_fval.item())\n","\n","# compute gradient of f at x\n","g = torch.autograd.grad(my_fval,x)[0]\n","\n","print(\"Gradient of f at x:\",g.data)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1663693505804,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"UX4zC76XlWr0"},"outputs":[],"source":["# Inherit from torch.autograd.Function\n","class My_Huber_Loss(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input, delta):\n","        m = (torch.abs(input)<=delta).float()\n","        ctx.save_for_backward(input,torch.tensor(m),torch.tensor(delta))\n","        output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        # retrieve saved tensors and use them in derivative calculation\n","        input, m, delta = ctx.saved_tensors\n","\n","        # Return Jacobian-vector producynt (chain rule)\n","        # For Huber loss function the Jacobian happens to be a diagonal matrix\n","        # Also, note that output_grad is a scalar, because forward function returns a scalar value\n","\n","        # Take our gradient we derived before and multiply it by the output_grad\n","        input_grad = (m*input + delta*(1.0-m)*(input/torch.abs(input))) * output_grad\n","\n","        # must return two gradients becuase forward function takes in two arguments\n","        return input_grad, None"]},{"cell_type":"markdown","metadata":{"id":"WkG5zXGZcgja"},"source":["# Gradient Descent on our Own Huber Loss"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":808,"status":"ok","timestamp":1663693506608,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"6DKnFDK0pPjF","outputId":"147e3204-4310-4cc6-ebec-e79e9a84a9d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Iteration: 0 Variable: [0.046 0.029 0.201 0.042 0.059 0.087 0.335 0.067 0.05  0.085] Loss: 0.390159\n","Iteration: 100 Variable: [0.005 0.003 0.948 0.004 0.006 0.007 0.008 0.006 0.005 0.007] Loss: 0.001521\n","Iteration: 200 Variable: [0.004 0.003 0.959 0.003 0.004 0.006 0.006 0.005 0.004 0.006]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"]},{"name":"stdout","output_type":"stream","text":[" Loss: 0.000919\n","Iteration: 300 Variable: [0.003 0.002 0.966 0.003 0.004 0.005 0.005 0.004 0.003 0.005] Loss: 0.000659\n","Iteration: 400 Variable: [0.003 0.002 0.97  0.003 0.003 0.004 0.005 0.004 0.003 0.004] Loss: 0.000514\n","Iteration: 500 Variable: [0.003 0.002 0.973 0.002 0.003 0.004 0.004 0.003 0.003 0.004] Loss: 0.000421\n","Iteration: 600 Variable: [0.002 0.002 0.975 0.002 0.003 0.003 0.004 0.003 0.002 0.003] Loss: 0.000356\n","Iteration: 700 Variable: [0.002 0.002 0.976 0.002 0.003 0.003 0.004 0.003 0.002 0.003] Loss: 0.000309\n","Iteration: 800 Variable: [0.002 0.001 0.978 0.002 0.002 0.003 0.003 0.003 0.002 0.003] Loss: 0.000272\n","Iteration: 900 Variable: [0.002 0.001 0.979 0.002 0.002 0.003 0.003 0.002 0.002 0.003] Loss: 0.000244\n"]}],"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(\"Target:\",y.numpy())\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,F.softmax,My_Huber_Loss.apply,y,1000,100)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663693506609,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"zn52-xK_PijV"},"outputs":[],"source":["# Inherit from Function\n","class My_softmax(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input):\n","        output = F.softmax(input,dim=0)\n","        ctx.save_for_backward(output) # this is the only tensor you will need to save for backward function\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        # retrieve saved tensors and use them in derivative calculation\n","        output = ctx.saved_tensors[0]\n","        \n","        # Create an identity matrix to represent the i==j case\n","        eye = torch.eye(output.shape[0])\n","\n","        # Subtract each S(1), S(2), .. from each row to account for both cases \n","        eye.sub_(torch.reshape(output,(-1,1)))\n","    \n","        # Sum the product row by row \n","        input_grad = torch.sum(output * eye * output_grad, dim=1)\n","        return input_grad"]},{"cell_type":"markdown","metadata":{"id":"fcixVFs4cwHO"},"source":["# Gradient Descent on our own Huber Loss and your own softmax"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836,"status":"ok","timestamp":1663693507441,"user":{"displayName":"Youssef Ismail","userId":"04729027483896348571"},"user_tz":360},"id":"UejqQeb4RZk0","outputId":"0cb590d1-3c8f-4f05-bf8f-48641d740220"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n","tensor([-2.1823, -0.4421,  0.2447,  1.6455, -1.3325,  0.7894, -1.2010, -1.5473,\n","        -0.3125, -0.5362], requires_grad=True)\n","Iteration: 0 Variable: [0.01  0.056 0.111 0.45  0.023 0.191 0.026 0.018 0.064 0.051] Loss: 0.520559\n","Iteration: 100 Variable: [0.001 0.007 0.951 0.008 0.003 0.01  0.004 0.003 0.007 0.006] Loss: 0.001366\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"]},{"name":"stdout","output_type":"stream","text":["Iteration: 200 Variable: [0.001 0.005 0.961 0.006 0.003 0.008 0.003 0.002 0.006 0.005] Loss: 0.000853\n","Iteration: 300 Variable: [0.001 0.005 0.967 0.005 0.002 0.007 0.002 0.002 0.005 0.004] Loss: 0.000621\n","Iteration: 400 Variable: [0.001 0.004 0.971 0.005 0.002 0.006 0.002 0.002 0.004 0.004] Loss: 0.000488\n","Iteration: 500 Variable: [0.001 0.004 0.973 0.004 0.002 0.005 0.002 0.001 0.004 0.003] Loss: 0.000402\n","Iteration: 600 Variable: [0.001 0.003 0.975 0.004 0.002 0.005 0.002 0.001 0.004 0.003] Loss: 0.000341\n","Iteration: 700 Variable: [0.001 0.003 0.977 0.004 0.002 0.005 0.002 0.001 0.003 0.003] Loss: 0.000297\n","Iteration: 800 Variable: [0.001 0.003 0.979 0.003 0.001 0.004 0.002 0.001 0.003 0.003] Loss: 0.000262\n","Iteration: 900 Variable: [0.001 0.003 0.98  0.003 0.001 0.004 0.002 0.001 0.003 0.003] Loss: 0.000235\n"]}],"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(y)\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","print(x)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,My_softmax.apply,My_Huber_Loss.apply,y,1000,100)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
